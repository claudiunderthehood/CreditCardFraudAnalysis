{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb6393c-f881-45e9-827f-35c968c7ab0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Credit Card Fraud Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7e67b-b254-41a8-aeba-97af76a4fc8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Project purpose\n",
    "\n",
    "The presented project concerns the analysis of all credit card transactions of the day, eventually discerning fraudulent actions that may lead to investigating further frauds. The transactions get generated and then injected into a pipeline that makes use of the Linear Regression algorithm to predict the possible fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bed0aa-f6db-4472-81b4-2b1e02f5be17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What's the point of a credit card analyser? \n",
    "\n",
    "As of the most recent data available, the global financial impact of credit card fraudulent activities continues to reach significant figures, totaling tens of billions of dollars. From a European perspective, credit card fraud remains a prevalent concern. In a recent report by the European Central Bank (ECB), it was revealed that the cumulative losses due to card fraud in the Single European Payment Area (SEPA) reached €1.8 billion in 2018. This underscores the substantial financial ramifications of fraudulent activities in the realm of electronic payments. Notably, the scale of the issue extends beyond national borders, affecting a considerable number of individuals. Recent statistics suggest that one in ten Europeans may have fallen victim to credit card fraud, with a median loss amounting to €399. These findings emphasize the ongoing challenges posed by credit card fraud and the importance of robust measures to safeguard financial transactions in the European context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7b5b4-07e4-4311-ba29-00cc1a5800ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How to read prediction results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff469a68-643a-46ef-8f69-9c82a57bd76d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "0 => No fraud Detected\n",
    "1 => Fraud Detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b28e64-7324-45f9-af85-d48bbb122090",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![1 or 0](pics/1_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8bb481-1632-49c5-aa71-74e8eac1a07d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674736b-a679-4002-9990-91fe14a855c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Data Pipeline](pics/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860165c-c96f-42af-b811-1e1d101c9287",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Transactions generator\n",
    "\n",
    "Regarding how these transactions get generated, it is all stored in the transactions generator directory. It is constructed through modules that will generate customers, terminals, add frauds and even generate the dataset. The frauds are generated by an overall consideration of some scenarios but for this project it has been choosen to follow some standard scenarios, as an instance, when the transaction amount surpasses a limit of 120. If a transaction falls in this scenario, then it has to be considered as a fraud. To emulate a more realistic situation, the features have been censored. When the censored dataset is generated, through HTTP POST, the dataframe will be sent as json data to logstash, which will listen the port 5044. The transaction generator generates transactions of the current day, and dumps them to logstash every day at 23:59. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276806b7-898f-4f55-80fe-13aefa2e1ac7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Logstash\n",
    "\n",
    "Logstash is the first checkpoint of the pipeline which is an interface for kafka and the Elastic suite. It is used for ingesting the data from the transaction generator; To set up logstash to accomodate this situation, the logstash.conf has been modeled to receive data through HTTP as an input listening to port 5044. Then it was necessary to check whether the data received was actually a json or a csv that got converted into a json. The final step of this process is to generate a csv file from the data received renaming the columns, assigning the features the respective labels. Then this csv file has to be sent to kafka. To prevent logstash from entering in a loop where it reads and duplicates the data, it has been used a peculiar workaround, which was a .log file that forced logstash into a reading mode of the csv file, producing this log file at the end of this process so that logstash could know that the process has ended. This is the section that has been just explained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e0023-8fef-4b7a-bcb1-d69549adac9f",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "input {\n",
    "  file {\n",
    "    path => \"/usr/share/logstash/transactions.csv\"\n",
    "    start_position => \"beginning\"\n",
    "    sincedb_path => \"/dev/null\"\n",
    "    mode => \"read\"\n",
    "    file_completed_action => \"log\"\n",
    "    file_completed_log_path => \"/usr/share/logstash/completed.log\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e1094-9c0e-44f2-abcd-81cfd0c9da02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Kafka\n",
    "\n",
    "Apache Kafka stands as an open-source distributed event streaming platform, offering robust support for high-performance data pipelines, streaming analytics, seamless data integration, and the execution of mission-critical applications. In this context, in the cluster, a topic named \"transactions\" is created to store what has been received from logstash and then sent to the consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37bac7-fe63-4bf6-88ee-c00cf1ff552a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Spark\n",
    "\n",
    "Apache Spark serves as an open-source unified analytics engine designed for large-scale data processing, providing a versatile solution for analytics, machine learning, and big data processing tasks. In this scenario, it will be used to apply the Linear Regression to create a prediction column which will tell us whether a transaction is fraudulent or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adbbbd8-df5d-4220-9a33-5db237f147db",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_features=['TX_AMOUNT','TX_FRAUD']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=input_features, \n",
    "                                outputCol='features')\n",
    "\n",
    "regression= LogisticRegression(featuresCol= 'features', \n",
    "                                    labelCol='TX_FRAUD')\n",
    "pipeline=Pipeline(stages=[assembler, regression])\n",
    "pipelineFit= pipeline.fit(train_set)\n",
    "updated_train_set = pipelineFit.transform(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42544a3a-26bf-4098-a125-0217c5d25ffc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Spark streaming\n",
    "\n",
    "Spark streaming processes the data that is being received from Kafka and then sends it to Elasticsearch. The first step is to read the data from kafka and to subscribe to the \"transactions\" topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d808c-e327-4567-9504-db3b0cb3f0f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafkaServer) \\\n",
    "  .option(\"subscribe\", topic) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723347f9-a5a5-49c6-95aa-50c5561ffc49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The received data will be, fit into a schema, elaborated and then written into the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff8842-fab5-4405-9431-0b885031e9d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", dataKafka).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process) \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c6707-8ac4-4b51-94df-24d29f2297cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The \"process\" method will just invoke the model to the data, convert \"TX_DATETIME\" from unix timestamp to standard datetime, and send the data to Elasticsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0ff81-614c-44c6-9bd9-f9f7d24dd2ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process(batch_df: DataFrame, batch_id: int):\n",
    "\n",
    "    if not batch_df.rdd.isEmpty():\n",
    "        print(\"----------------- \\n Processing\\n\")\n",
    "        batch_df.show()\n",
    "\n",
    "        batch_df = batch_df.withColumn('TX_DATETIME', \n",
    "                            from_unixtime(col('TX_DATETIME') / 1000))\n",
    "        data2=pipelineFit.transform(batch_df)\n",
    "        data2.summary()\n",
    "        data2.show()\n",
    "\n",
    "        print(\"--------------------- \\nSending data to ES \\n\")\n",
    "        data2.select(\"TRANSACTION_ID\", \"TX_DATETIME\", \"CUSTOMER_ID\", \n",
    "                     \"TERMINAL_ID\", \"TX_AMOUNT\",\n",
    "                     \"TX_TIME_SECONDS\", \"TX_DURING_NIGHT\", \n",
    "                     \"@timestamp\", \"prediction\") \\\n",
    "        .write \\\n",
    "        .format(\"org.elasticsearch.spark.sql\") \\\n",
    "        .mode('append') \\\n",
    "        .option(\"es.mapping.id\",\"TRANSACTION_ID\") \\\n",
    "        .option(\"es.nodes\", elastic_host).save(elastic_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa03cca-6f92-4416-b2d1-4aeb0febf0e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Elasticsearch and Kibana\n",
    "\n",
    "Elasticsearch stands as a distributed, open-source search, and analytics engine designed for diverse data types, providing a powerful solution for effective data exploration and analysis. The Spark Stream node effectively correlates the index based on the information and code supplied in the script. The data will be visualized and organized thanks to Kibana:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff11ad9-dad6-4248-a46a-6c691ca6f31e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Graphs](pics/graphs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79977b-dd55-4d9a-8dd0-020df47b2684",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Meme](pics/meme1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
